{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Redes Convolucionales sobre imágenes\n",
    "---\n",
    "Las redes neuronales hoy en día han sido extendidas a numerosas aplicaciones gracias a la arquitectura definida para cada tipo de problema. Las redes neuronales que aplican la operación de convolución [[3]](#refs) o convoluciones en sus capas son concidas como *CNN* o *ConvNets*, lo cual se especializa en trabajar en datos con forma matricial (ya sea bi-dimensional o tri-dimensional), lo cual se adecúa perfectamente a imágenes (matrices), ya que gracias a su conectividad local se especializan en reconocer patrones sobre los datos de manera espacial, como refleja la siguiente imagen:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*N4h1SgwbWNmtrRhszM9EJg.png\" title=\"Title text\" width=\"90%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajará con un extracto bastante pequeño del dataset conocido como **101-Food**[[4]](#refs), el cual consta de mil imágenes pertenecientes a 3 clases (*Hambuger, Hot Dog* y *Pizza*) separados en conjunto de entrenamiento y validación.  \n",
    "El extracto pequeño del dataset con el que se trabajará deberá ser descargado del siguiente __[link](https://www.dropbox.com/s/56xqazmhbh0doi7/food_data.zip?dl=0)__ a través de Dropbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">a) Construya funciones para leer los datos y cargarlos al momento de entrenar (durante cada epoch), para ésto utilice Image Data Generator de keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #no transformation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'food_data/train',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'food_data/val',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">b) Utilice la red tradicional (Feed Forward) entregada en el código para ser entrenada sobre los datos vectorizados, esto es que cada imagen queda representada como un vector gigante, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica accuracy sobre el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 67500)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 67500)             270000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               17280256  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 17,583,539\n",
      "Trainable params: 17,448,539\n",
      "Non-trainable params: 135,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 78s 1s/step - loss: 9.2074 - acc: 0.4075 - val_loss: 9.3745 - val_acc: 0.4149\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 43s 569ms/step - loss: 9.5097 - acc: 0.4042 - val_loss: 9.0859 - val_acc: 0.4313\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 41s 550ms/step - loss: 9.4001 - acc: 0.4121 - val_loss: 9.2522 - val_acc: 0.4190\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 41s 552ms/step - loss: 9.5215 - acc: 0.4054 - val_loss: 9.7053 - val_acc: 0.3979\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 42s 554ms/step - loss: 9.7531 - acc: 0.3925 - val_loss: 10.4144 - val_acc: 0.3539\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 42s 566ms/step - loss: 9.9122 - acc: 0.3837 - val_loss: 9.8714 - val_acc: 0.3856\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 41s 548ms/step - loss: 9.6544 - acc: 0.3992 - val_loss: 10.0742 - val_acc: 0.3750\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 41s 549ms/step - loss: 9.5624 - acc: 0.4058 - val_loss: 10.0968 - val_acc: 0.3732\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 42s 561ms/step - loss: 9.5110 - acc: 0.4083 - val_loss: 9.9186 - val_acc: 0.3820\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 41s 546ms/step - loss: 9.4172 - acc: 0.4142 - val_loss: 8.6009 - val_acc: 0.4577\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 42s 566ms/step - loss: 8.8279 - acc: 0.4496 - val_loss: 9.3843 - val_acc: 0.4137\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 42s 565ms/step - loss: 9.0141 - acc: 0.4387 - val_loss: 9.6164 - val_acc: 0.4014\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 42s 563ms/step - loss: 9.0039 - acc: 0.4383 - val_loss: 9.2839 - val_acc: 0.4225\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 43s 576ms/step - loss: 9.1300 - acc: 0.4312 - val_loss: 8.9076 - val_acc: 0.4454\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 44s 584ms/step - loss: 9.1354 - acc: 0.4321 - val_loss: 9.5347 - val_acc: 0.4085\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 41s 550ms/step - loss: 9.3472 - acc: 0.4192 - val_loss: 9.2139 - val_acc: 0.4278\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 43s 573ms/step - loss: 9.1832 - acc: 0.4300 - val_loss: 9.2221 - val_acc: 0.4278\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 42s 554ms/step - loss: 8.9934 - acc: 0.4408 - val_loss: 9.2594 - val_acc: 0.4243\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 43s 567ms/step - loss: 9.1490 - acc: 0.4317 - val_loss: 9.1498 - val_acc: 0.4296\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 48s 637ms/step - loss: 9.1317 - acc: 0.4317 - val_loss: 9.2886 - val_acc: 0.4219\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 50s 660ms/step - loss: 9.4074 - acc: 0.4146 - val_loss: 10.2498 - val_acc: 0.3627\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 43s 571ms/step - loss: 9.2141 - acc: 0.4271 - val_loss: 9.4212 - val_acc: 0.4155\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 42s 556ms/step - loss: 8.8911 - acc: 0.4475 - val_loss: 9.3077 - val_acc: 0.4225\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 46s 608ms/step - loss: 9.1457 - acc: 0.4321 - val_loss: 9.6198 - val_acc: 0.4032\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 64s 848ms/step - loss: 9.1956 - acc: 0.4288 - val_loss: 8.9983 - val_acc: 0.4401\n",
      "('Accuracy validation: ', 0.42333333373069765)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_generator.image_shape)) #full dense\n",
    "model.add(BatchNormalization()) #to normalize the input..\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128,activation='relu')) #128\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)\n",
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  55, 145],\n",
       "       [  0,  51, 149],\n",
       "       [  0,  63, 137]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "Y_pred = model.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "confusion_matrix(validation_generator.classes, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el modelo solo funciona un poco mejor que una función aleatoria, por lo cual no nos satisface y podemos ver que la mayoria de las predicciones que hace es pizza y un porcentaje se lo lleva hot dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">c) Utilice la red convolucional (CNN) entregada en el código para ser entrenada sobre los datos brutos, matrices RGB de píxeles, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica accuracy sobre el conjunto de validación. Compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 328s 4s/step - loss: 1.2736 - acc: 0.3975 - val_loss: 1.0487 - val_acc: 0.4670\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 352s 5s/step - loss: 1.0538 - acc: 0.4625 - val_loss: 1.0101 - val_acc: 0.5299\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 341s 5s/step - loss: 1.0235 - acc: 0.4925 - val_loss: 0.9886 - val_acc: 0.5053\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 332s 4s/step - loss: 0.9895 - acc: 0.5150 - val_loss: 0.9509 - val_acc: 0.5528\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 328s 4s/step - loss: 0.9597 - acc: 0.5429 - val_loss: 0.9339 - val_acc: 0.5423\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 364s 5s/step - loss: 0.9432 - acc: 0.5646 - val_loss: 0.9607 - val_acc: 0.5493\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 351s 5s/step - loss: 0.9051 - acc: 0.5863 - val_loss: 0.8604 - val_acc: 0.6232\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 354s 5s/step - loss: 0.8936 - acc: 0.5975 - val_loss: 0.8617 - val_acc: 0.6021\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 342s 5s/step - loss: 0.8471 - acc: 0.6262 - val_loss: 0.7980 - val_acc: 0.6320\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 353s 5s/step - loss: 0.8281 - acc: 0.6379 - val_loss: 1.0566 - val_acc: 0.5863\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 344s 5s/step - loss: 0.8122 - acc: 0.6471 - val_loss: 0.8037 - val_acc: 0.6637\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 329s 4s/step - loss: 0.7686 - acc: 0.6654 - val_loss: 0.9206 - val_acc: 0.5845\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.7797 - acc: 0.6646 - val_loss: 0.8998 - val_acc: 0.6056\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 328s 4s/step - loss: 0.7581 - acc: 0.6817 - val_loss: 0.7487 - val_acc: 0.6655\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.7292 - acc: 0.6958 - val_loss: 1.2394 - val_acc: 0.5035\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 345s 5s/step - loss: 0.7166 - acc: 0.7037 - val_loss: 0.7219 - val_acc: 0.6972\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 359s 5s/step - loss: 0.6889 - acc: 0.7154 - val_loss: 1.0779 - val_acc: 0.5475\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 330s 4s/step - loss: 0.6972 - acc: 0.7150 - val_loss: 0.6646 - val_acc: 0.7095\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 333s 4s/step - loss: 0.7171 - acc: 0.7092 - val_loss: 0.6952 - val_acc: 0.7201\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 334s 4s/step - loss: 0.6512 - acc: 0.7383 - val_loss: 0.6975 - val_acc: 0.7014\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 334s 4s/step - loss: 0.6625 - acc: 0.7296 - val_loss: 0.7499 - val_acc: 0.7130\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 335s 4s/step - loss: 0.6969 - acc: 0.7212 - val_loss: 0.7338 - val_acc: 0.7130\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 333s 4s/step - loss: 0.6433 - acc: 0.7292 - val_loss: 0.6767 - val_acc: 0.7042\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.6434 - acc: 0.7246 - val_loss: 0.6904 - val_acc: 0.7148\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.6340 - acc: 0.7488 - val_loss: 0.6924 - val_acc: 0.6919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fc0c14210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=train_generator.image_shape,activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(len(train_generator.class_indices),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que el accuracy aumenta considerablemente entre el Feed-Foward y el CNN, esto es debido a que el Feed-Foward es una red neuronal que funciona de una manera bastante simple y no deja representar bien la imagen, en cambio el CNN permite realizar una mejor representación de la imagen, al representarlo matricialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56, 81, 63],\n",
       "       [42, 91, 67],\n",
       "       [46, 86, 68]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "Y_pred = model.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "confusion_matrix(validation_generator.classes, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la matriz de confusión para posteriormente compararla con la que se realiza al intercambiar datos del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Genere un conjunto datos con incorrecta etiquetación de manera manual y vea si el modelo convolucional se sigue comportando de la misma manera. Para esto tome 100 imágenes aleatorias de entrenamiento de la carpeta hot dog y 100 imágenes aleatorias de entrenamiento de la carpeta hamburger e intercambielas, sin manipular las imágenes de la carpeta pizza y con el conjunto de validación intacto. Genere las matrices de confusión en el conjunto de validación para visualizar cómo afectó al modelo la corrupción realizada a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #no transformation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'food_data/train',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'food_data/val',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 340s 5s/step - loss: 1.2403 - acc: 0.3671 - val_loss: 1.0797 - val_acc: 0.4896\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 323s 4s/step - loss: 1.0768 - acc: 0.4113 - val_loss: 1.0606 - val_acc: 0.5053\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 1.0528 - acc: 0.4487 - val_loss: 0.9879 - val_acc: 0.5370\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 1.0323 - acc: 0.4921 - val_loss: 0.9630 - val_acc: 0.5264\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.9840 - acc: 0.5317 - val_loss: 0.9545 - val_acc: 0.5158\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.9528 - acc: 0.5438 - val_loss: 1.0051 - val_acc: 0.5088\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.9383 - acc: 0.5367 - val_loss: 0.9281 - val_acc: 0.5739\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 0.9206 - acc: 0.5692 - val_loss: 0.9134 - val_acc: 0.5528\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 326s 4s/step - loss: 0.8845 - acc: 0.5787 - val_loss: 0.8447 - val_acc: 0.5687\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 0.8698 - acc: 0.5883 - val_loss: 1.1210 - val_acc: 0.4648\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 325s 4s/step - loss: 0.8703 - acc: 0.5996 - val_loss: 0.8674 - val_acc: 0.5739\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 0.8323 - acc: 0.6229 - val_loss: 0.7838 - val_acc: 0.6356\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 325s 4s/step - loss: 0.8363 - acc: 0.6208 - val_loss: 0.7795 - val_acc: 0.6180\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 325s 4s/step - loss: 0.8037 - acc: 0.6254 - val_loss: 0.8312 - val_acc: 0.6092\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 323s 4s/step - loss: 0.7835 - acc: 0.6492 - val_loss: 0.7730 - val_acc: 0.6109\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.7695 - acc: 0.6567 - val_loss: 0.7755 - val_acc: 0.6602\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 322s 4s/step - loss: 0.7680 - acc: 0.6525 - val_loss: 0.7383 - val_acc: 0.6514\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.7698 - acc: 0.6521 - val_loss: 0.8315 - val_acc: 0.6303\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 326s 4s/step - loss: 0.7454 - acc: 0.6846 - val_loss: 0.8149 - val_acc: 0.6320\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 321s 4s/step - loss: 0.7228 - acc: 0.6729 - val_loss: 0.7765 - val_acc: 0.6372\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 327s 4s/step - loss: 0.7134 - acc: 0.6871 - val_loss: 0.6665 - val_acc: 0.6690\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 324s 4s/step - loss: 0.7149 - acc: 0.6829 - val_loss: 0.7015 - val_acc: 0.6778\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 326s 4s/step - loss: 0.7164 - acc: 0.6871 - val_loss: 0.7599 - val_acc: 0.6479\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 322s 4s/step - loss: 0.6939 - acc: 0.7000 - val_loss: 0.6836 - val_acc: 0.6972\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 325s 4s/step - loss: 0.6935 - acc: 0.6871 - val_loss: 0.7542 - val_acc: 0.6444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e044e1750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=train_generator.image_shape,activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(len(train_generator.class_indices),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el accuracy disminuye en un 0.05 aproximadamente, lo cual es menos de lo esperado, ya que se cambian 1/8 de dos categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 79, 81],\n",
       "       [43, 65, 92],\n",
       "       [48, 74, 78]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "Y_pred = model.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "confusion_matrix(validation_generator.classes, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui los datos que nos gustarian que se mantuvieran altos en la diagonal, pero se ve que en los dos primeros terminos de esta disminuye, lo cual es de esperarse debido a que fueron en la dos primeras categorias que se intervinieron los datos(hamburguesas y hot dogs), mientras el tercer termino de la columna , esto se puede deber a que ahora las pizzas son mas facil reconocibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
