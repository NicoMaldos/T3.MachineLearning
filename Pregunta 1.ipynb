{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Calidad de un vino\n",
    "---\n",
    "\n",
    "Existen muchas variedades de vino existentes debido a los distintos gustos que tienen las personas. Del gusto se desprende la calidad que una persona le podría asignar a un vino, el cual proviene del gusto de la persona en particular, o bien, a la gran cantidad de quı́micos y procesos que se aplican a la producción de vino. Para el área de negocios, el estimar cuál es la calidad de un vino en base a la apreciación del público es una tarea bastante difı́cil.  \n",
    "Para esta actividad se trabajará con dos *datasets* asociados a las variantes tinto y blanco del vino portugués\n",
    "”Vinho Verde”[[1]](#refs). Debido a temas privados solo se cuenta con las caracterı́stcas fisioquı́micas asociadas a un\n",
    "vino en particular, los cuales corresponden a 11 atributos numéricos descritos en el siguiente __[link](http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names)__.\n",
    "\n",
    "Este problema puede ser abordado como clasificación de 11 clases o de regresión, ya que el atributo a estimar,\n",
    "*quality*, tiene un dominio como valor entero 0 y 10. La forma de resolverlo será a través de **ensamblados**.\n",
    "\n",
    "<img src=\"https://uploads.toptal.io/blog/image/92064/toptal-blog-image-1454584112948-fc1d35939aa1886bf30c816b3ac20e21.jpg\" title=\"Title text\" width=\"20%\"  />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) Carge los dos dataset en un único dataframe de pandas, además de agregar una columna indicando si es vino tinto o blanco. Describa el dataset a trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_red = pd.read_csv(\"winequality-red.csv\",sep=\";\").assign(white= lambda x: 0, red = lambda x: 1)\n",
    "df_white = pd.read_csv(\"winequality-white.csv\",sep=\";\").assign(white= lambda x: 1, red = lambda x: 0)\n",
    "df = pd.concat([df_red,df_white], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "\n",
       "   alcohol  quality  red  white  \n",
       "0      9.4        5    1      0  \n",
       "1      9.8        5    1      0  \n",
       "2      9.8        5    1      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.038</td>\n",
       "      <td>59.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>8.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.051</td>\n",
       "      <td>46.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.050</td>\n",
       "      <td>17.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "1319            6.4              0.25         0.53             6.6      0.038   \n",
       "2023            6.3              0.21         0.28             1.5      0.051   \n",
       "1577            7.1              0.64         0.49             1.8      0.050   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "1319                 59.0                 234.0   0.9955  3.03       0.42   \n",
       "2023                 46.0                 142.0   0.9928  3.23       0.42   \n",
       "1577                 17.0                 128.0   0.9946  3.31       0.58   \n",
       "\n",
       "      alcohol  quality  red  white  \n",
       "1319      8.8        5    0      1  \n",
       "2023     10.1        6    0      1  \n",
       "1577     10.6        4    0      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.215307</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>5.443235</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218501</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>10.491801</td>\n",
       "      <td>5.818378</td>\n",
       "      <td>0.246114</td>\n",
       "      <td>0.753886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.296434</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>4.757804</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160787</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>1.192712</td>\n",
       "      <td>0.873255</td>\n",
       "      <td>0.430779</td>\n",
       "      <td>0.430779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    6497.000000       6497.000000  6497.000000     6497.000000   \n",
       "mean        7.215307          0.339666     0.318633        5.443235   \n",
       "std         1.296434          0.164636     0.145318        4.757804   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  6497.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056034            30.525319            115.744574     0.994697   \n",
       "std       0.035034            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality          red  \\\n",
       "count  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000   \n",
       "mean      3.218501     0.531268    10.491801     5.818378     0.246114   \n",
       "std       0.160787     0.148806     1.192712     0.873255     0.430779   \n",
       "min       2.720000     0.220000     8.000000     3.000000     0.000000   \n",
       "25%       3.110000     0.430000     9.500000     5.000000     0.000000   \n",
       "50%       3.210000     0.510000    10.300000     6.000000     0.000000   \n",
       "75%       3.320000     0.600000    11.300000     6.000000     0.000000   \n",
       "max       4.010000     2.000000    14.900000     9.000000     1.000000   \n",
       "\n",
       "             white  \n",
       "count  6497.000000  \n",
       "mean      0.753886  \n",
       "std       0.430779  \n",
       "min       0.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) Se pueden observar las siguientes carácterísticas químicas de los vinos (inputs):\n",
    "    1) fixed acidity\n",
    "    2) volatile acidity\n",
    "    3) citric acid\n",
    "    4) residual sugar\n",
    "    5) chlorides\n",
    "    6) free sulfur dioxide\n",
    "    7) total sulfur dioxide\n",
    "    8) density\n",
    "    9) pH\n",
    "    10) sulphates\n",
    "    11) alcohol\n",
    "    12) red\n",
    "    13) white\n",
    "II) Se puede observar la característica que se quiere predecir (output):\n",
    "    1) quality: el mínimo es 3 y el máximo es 9\n",
    "\n",
    "El dataframe está muy ordenado. Arriba quedaron todos los vinos rojos y abajo todos los blancos. La razón de lo anterior es el mecanísmo de concatenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">b) Aborde este problema como si fuera de clasificación con multiples clases para predecir el valor de calidad de un vino, es decir, utilice las distintas caracterı́sticas fisioquı́micas presentes en los datos para estimar la etiqueta ¿Cuántas clases son y cuántos ejemplos hay por clase? ¿Qué sucede con predecir si un vino tiene calidad mínima (0) o máxima(10)? Además para el propósito académico de esta actividad cree un conjunto de pruebas (20%) para evaluar la generalización final del modelo y otro de validación (20%) si estima conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6497\n"
     ]
    }
   ],
   "source": [
    "print df.quality.nunique()\n",
    "print df.quality.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2836\n",
       "5    2138\n",
       "7    1079\n",
       "4     216\n",
       "8     193\n",
       "3      30\n",
       "9       5\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí nos damos cuenta que la representación de calidades es precisa. Hay 7 categorías utilizadas.\n",
    "3, 4, 5, 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hay 6497 datos.\n",
    "import numpy as np\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_val = df[~msk]\n",
    "\n",
    "    \n",
    "x_train = df_train.drop(columns=['quality',])\n",
    "y_train = df_train[\"quality\"]\n",
    "\n",
    "x_test = df_val.drop(columns=['quality',])\n",
    "y_test = df_val[\"quality\"]\n",
    "#y = df[\"quality\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante notar que se separaron random los conjuntos de entrenamiento y de prueba, logrando evitar que todos los blancos y tintos queden juntos entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity           5193\n",
      "volatile acidity        5193\n",
      "citric acid             5193\n",
      "residual sugar          5193\n",
      "chlorides               5193\n",
      "free sulfur dioxide     5193\n",
      "total sulfur dioxide    5193\n",
      "density                 5193\n",
      "pH                      5193\n",
      "sulphates               5193\n",
      "alcohol                 5193\n",
      "red                     5193\n",
      "white                   5193\n",
      "dtype: int64\n",
      "5193\n",
      "**********\n",
      "fixed acidity           1304\n",
      "volatile acidity        1304\n",
      "citric acid             1304\n",
      "residual sugar          1304\n",
      "chlorides               1304\n",
      "free sulfur dioxide     1304\n",
      "total sulfur dioxide    1304\n",
      "density                 1304\n",
      "pH                      1304\n",
      "sulphates               1304\n",
      "alcohol                 1304\n",
      "red                     1304\n",
      "white                   1304\n",
      "dtype: int64\n",
      "1304\n"
     ]
    }
   ],
   "source": [
    "print x_train.count()\n",
    "print y_train.count()\n",
    "print \"**********\"\n",
    "print x_test.count()\n",
    "print y_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se podrán predecir vinos de calidad mínima (0) ni máxima (10) ya que la máquina no tiene manera de aprender que existen. \n",
    "El modelo se puede reparar fácilmente usando algún dataset extra que permita incluir en el modelo de aprendizaje los parámetros que definen un vino como 0, 1, 2 o 10 como categoría.\n",
    "Por otro lado si obtuvieramos de alguna manera la combinación de parámetros que definen un vino perfecto (10) y uno malo (0) podríamos generar en base a calculos estadísticos una aproximación de modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">c) Entrene un solo Árbol de Clasificación de múltiples niveles para resolver el problema. Defina un Árbol no regularizado (como el que no tiene límites en su profundidad) y otro Árbol regularizado (variando los hiper-parámetros que prefiera, por ejemplo, los más comunes como la profundidad, el número mínimo de datos para realizar split o el número mínimo de datos en cada hoja), recuerde que las decisiones no pueden ser basadas mirando el conjunto de pruebas. Debido al desbalanceo que se produce en las clases mida la métrica F1-score [2] sobre el conjunto de entrenamiento y de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score: ', 0.6035276073619632)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "modelT = Tree(max_depth=10000) \n",
    "modelT.fit(x_train,y_train.values)\n",
    "PrediccionT=modelT.predict(x_test)\n",
    "print(\"f1.score: \",f1_score(y_true=y_test,y_pred=PrediccionT,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Entrene un ensamblado de árboles de múltiples niveles, mediante la técnica de Bagging, compare el Árbol no regularizado con el regularizado (seteando los hiper-parámetros en base a lo experimentado anteriormente en c)) ¿Qué debería suceder? ¿Se visualiza overfitting? Varíe la cantidad de árboles de decisión utilizados en el ensamblado (n estimators), realice un gráfico resumen del F1-score de entrenamiento y de pruebas en función de este hiper-parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.68174846625766872)\n"
     ]
    }
   ],
   "source": [
    "modelB_1000 = BaggingClassifier(base_estimator=Tree(max_depth=10000), n_estimators=1000, n_jobs=-1)\n",
    "modelB_1000.fit(x_train,y_train.values)\n",
    "y_predB_1000 = modelB_1000.predict(x_train)\n",
    "PrediccionB_1000=modelB_1000.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predB_1000,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionB_1000,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.68328220858895705)\n"
     ]
    }
   ],
   "source": [
    "#con n=100\n",
    "modelB_100 = BaggingClassifier(base_estimator=Tree(max_depth=10000), n_estimators=100, n_jobs=-1)\n",
    "modelB_100.fit(x_train,y_train.values)\n",
    "y_predB_100 = modelB_100.predict(x_train)\n",
    "PrediccionB_100=modelB_100.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predB_100,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionB_100,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 0.98420951280570002)\n",
      "('f1.score test: ', 0.65414110429447858)\n"
     ]
    }
   ],
   "source": [
    "#con n=10\n",
    "modelB_10 = BaggingClassifier(base_estimator=Tree(max_depth=10000), n_estimators=10, n_jobs=-1)\n",
    "modelB_10.fit(x_train,y_train.values)\n",
    "y_predB_10 = modelB_10.predict(x_train)\n",
    "PrediccionB_10=modelB_10.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predB_10,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionB_10,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 0.84228769497400335)\n",
      "('f1.score test: ', 0.57055214723926384)\n"
     ]
    }
   ],
   "source": [
    "#con n=1\n",
    "modelB_1 = BaggingClassifier(base_estimator=Tree(max_depth=10000), n_estimators=1, n_jobs=-1)\n",
    "modelB_1.fit(x_train,y_train.values)\n",
    "y_predB_1 = modelB_1.predict(x_train)\n",
    "PrediccionB_1=modelB_1.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predB_1,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionB_1,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El n estimator más efectivo es n=1000. Se aprecia overfitting para casi todos los n estimators probados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">e) Entrene un ensamblado de árboles de múltiples niveles, mediante la técnica de AdaBoost, compare el Árbol no regularizado con el regularizado (seteando los hiper-parámetros en base a lo experimentado anteriormente en c) ¿Se visualiza overfitting? ¿Qué técnica se utiliza, re-muestrear o pesar ejemplos? ¿Qué le parece más sensato?. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (n estimators), realice un gráfico resumen del F1-score de entrenamiento y de pruebas en función de este hiper-parámetro. Compare y analice con la técnica utilizada en d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.60582822085889576)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "modelA_1000 = AdaBoostClassifier(base_estimator=Tree(max_depth=10000), n_estimators=1000)\n",
    "modelA_1000.fit(x_train,y_train.values)\n",
    "y_predA_1000= modelA_1000.predict(x_train)\n",
    "PrediccionA_1000=modelA_1000.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predA_1000,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionA_1000,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.60582822085889576)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "modelA_100 = AdaBoostClassifier(base_estimator=Tree(max_depth=10000), n_estimators=100)\n",
    "modelA_100.fit(x_train,y_train.values)\n",
    "y_predA_100= modelA_100.predict(x_train)\n",
    "PrediccionA_100=modelA_100.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predA_100,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionA_100,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.60812883435582821)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "modelA_10 = AdaBoostClassifier(base_estimator=Tree(max_depth=10000), n_estimators=10)\n",
    "modelA_10.fit(x_train,y_train.values)\n",
    "y_predA_10= modelA_10.predict(x_train)\n",
    "PrediccionA_10=modelA_10.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predA_10,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionA_10,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.6073619631901841)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "modelA_1 = AdaBoostClassifier(base_estimator=Tree(max_depth=10000), n_estimators=1)\n",
    "modelA_1.fit(x_train,y_train.values)\n",
    "y_predA_1= modelA_1.predict(x_train)\n",
    "PrediccionA_1=modelA_1.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predA_1,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionA_1,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El n estimator más efectivo es n=1. Se aprecia overfitting para todos los n estimators probados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">f) Pruebe otra técnica de ensamblado dedicada a árboles de decisión, que combina el muestreo boostrap de Bagging con muestreo sobre las features: Random Forest, compare el Árbol no regularizado con el regularizado ¿Se visualiza overfitting?. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (n estimators), realice un gráfico resumen el F1-score de entrenamiento y de pruebas en función de este hiper-parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.68634969325153372)\n"
     ]
    }
   ],
   "source": [
    "modelF_1000 = RandomForestClassifier(n_estimators=1000, max_depth=10000,n_jobs=-1)\n",
    "modelF_1000.set_params(warm_start=True,oob_score=True)\n",
    "modelF_1000.fit(x_train,y_train.values)\n",
    "y_predF_1000= modelF_1000.predict(x_train)\n",
    "PrediccionF_1000=modelF_1000.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predF_1000,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_1000,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.68174846625766872)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelF_100 = RandomForestClassifier(n_estimators=100, max_depth=10000,n_jobs=-1)\n",
    "modelF_100.set_params(warm_start=True,oob_score=True)\n",
    "modelF_100.fit(x_train,y_train.values)\n",
    "y_predF_100= modelF_100.predict(x_train)\n",
    "PrediccionF_100=modelF_100.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predF_100,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_100,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldos/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/maldos/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 0.99017908723281345)\n",
      "('f1.score test: ', 0.64340490797546013)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelF_10 = RandomForestClassifier(n_estimators=10, max_depth=10000,n_jobs=-1)\n",
    "modelF_10.set_params(warm_start=True,oob_score=True)\n",
    "modelF_10.fit(x_train,y_train.values)\n",
    "y_predF_10= modelF_10.predict(x_train)\n",
    "PrediccionF_10=modelF_10.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predF_10,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_10,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 0.82938571153475826)\n",
      "('f1.score test: ', 0.51993865030674846)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelF_1= RandomForestClassifier(n_estimators=1, max_depth=10000,n_jobs=-1)\n",
    "modelF_1.set_params(warm_start=True,oob_score=True)\n",
    "modelF_1.fit(x_train,y_train.values)\n",
    "y_predF_1= modelF_1.predict(x_train)\n",
    "PrediccionF_1=modelF_1.predict(x_test)\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predF_1,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_1,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el n estimator más efectivo es n=100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">g) Verifique que el OOB error (out of bag error) de los ensambladores que utilizan la técnica boostrap puede ser una alternativa como métrica de generalización, compárelo con el error calculado sobre el conjunto de pruebas y validación (o en su defecto cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('OOB error: ', 0.31908338147506254)\n",
      "('f1.score test: ', 0.68174846625766872)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oob_error = 1 - modelF_100.oob_score_\n",
    "test_error = 1- modelF_100.score(x_test,y_test)\n",
    "#val_error = 1- model_forest.score(x_val,y_val)\n",
    "print(\"OOB error: \",oob_error)\n",
    "#print (\"Val error: \",val_error)\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_100,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El OOB representa el error de los subconjuntos con los datos no ingresados a este, y al compararlo con el test error podemos darnos cuenta como aporta a disminuir el error el usar muchos subconjuntos distintos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">h) Entrene alguna otra máquina de aprendizaje, elegida por usted de entre todas las vistas en el curso, para resolver el problema. Elija los hiper-parámetros que estime convenientes intentando aumentar el F1-score obtenido por los algoritmos anteriores ¿Se logra una mejora? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este problema usaremos un simple regresor lineal y diremos que su clasificación es la clase aplicando la función techo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 0.53610629693818601)\n",
      "('f1.score test: ', 0.51380368098159512)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_lineal = LinearRegression(fit_intercept=True, normalize=False)\n",
    "model_lineal.fit(x_train, y_train.values)\n",
    "y_predlin = model_lineal.predict(x_train)\n",
    "for i in range(len(y_predlin)):\n",
    "    if ( (y_predlin[i] - math.floor(y_predlin[i])) > 0.5 ):\n",
    "        y_predlin[i] = math.ceil(y_predlin[i])\n",
    "    else:\n",
    "        y_predlin[i] = math.floor(y_predlin[i])\n",
    "Prediccionlin=model_lineal.predict(x_test)\n",
    "for i in range(len(Prediccionlin)):\n",
    "    if ( (Prediccionlin[i] - math.floor(Prediccionlin[i])) > 0.5 ):\n",
    "        Prediccionlin[i] = math.ceil(Prediccionlin[i])\n",
    "    else:\n",
    "        Prediccionlin[i] = math.floor(Prediccionlin[i])\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predlin,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=Prediccionlin,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una simple regresión lineal no puede determinar el ranking aproximado de calidad de un vino. Pero para poder tener un mejor resultado luego de obtener su puntuación continua es encasillarla en función de su grado para que se predigan valores discretos por medio de la función piso o techo aplicadas.\n",
    "EJ: El modelo de regresión lineal predice un valor para \"y\" continuo tal que \"y\" quiere tomar el valor 4.3\n",
    "debemos discretizar el valor 4.3 por medio de la función piso para finalmente indicar que el vino tiene calidad 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">i) Compare y analice las distintas maneras con las que se resolvió el problema definido en b), por ejemplo incluya las decisiones que conlleva y los resultados que reflejan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuraccy\t\tmethod\n",
      "0.603527607362\tTree\n",
      "0.681748466258\tBagging n-estimator = 1000\n",
      "0.683282208589\tBagging n-estimator = 100\n",
      "0.654141104294\tBagging n-estimator = 10\n",
      "0.570552147239\tBagging n-estimator = 1\n",
      "0.605828220859\tAdaBoost n-estimator = 1000\n",
      "0.605828220859\tAdaBoost n-estimator = 100\n",
      "0.608128834356\tAdaBoost n-estimator = 10\n",
      "0.60736196319\tAdaBoost n-estimator = 1\n",
      "0.686349693252\tRandom forest n-estimator = 1000\n",
      "0.681748466258\tRandom forest n-estimator = 100\n",
      "0.643404907975\tRandom forest n-estimator = 10\n",
      "0.519938650307\tRandom forest n-estimator = 1\n",
      "0.513803680982\tLinear regression\n"
     ]
    }
   ],
   "source": [
    "print \"acuraccy\\t\\tmethod\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionT,average=\"micro\"))+\"\\t\"+\"Tree\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionB_1000,average=\"micro\"))+\"\\t\"+\"Bagging n-estimator = 1000\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionB_100,average=\"micro\"))+\"\\t\"+\"Bagging n-estimator = 100\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionB_10,average=\"micro\"))+\"\\t\"+\"Bagging n-estimator = 10\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionB_1,average=\"micro\"))+\"\\t\"+\"Bagging n-estimator = 1\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionA_1000,average=\"micro\"))+\"\\t\"+\"AdaBoost n-estimator = 1000\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionA_100,average=\"micro\"))+\"\\t\"+\"AdaBoost n-estimator = 100\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionA_10,average=\"micro\"))+\"\\t\"+\"AdaBoost n-estimator = 10\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionA_1,average=\"micro\"))+\"\\t\"+\"AdaBoost n-estimator = 1\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionF_1000,average=\"micro\"))+\"\\t\"+\"Random forest n-estimator = 1000\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionF_100,average=\"micro\"))+\"\\t\"+\"Random forest n-estimator = 100\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionF_10,average=\"micro\"))+\"\\t\"+\"Random forest n-estimator = 10\"\n",
    "print str(f1_score(y_true=y_test,y_pred=PrediccionF_1,average=\"micro\"))+\"\\t\"+\"Random forest n-estimator = 1\"\n",
    "print str(f1_score(y_true=y_test,y_pred=Prediccionlin,average=\"micro\"))+\"\\t\"+\"Linear regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor de los modelos es el random forest con n-estimator=100 seguido por el bagging con nestimator=1000, lo cual no nos sorprende, pero esperabamos que el adaboost tuviera mejor resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">j) Defina otra forma de combinar los valores que entregan los ensamblados al hacer predicciones y compare con lo que se hace actualmente, por ejemplo Bagging realiza el voto de la mayoría para clasificación y promedio para regresión, AdaBoost realiza una combinación ponderada de cada clasificador dependiendo de su habilidad (desempeño para clasificar el conjunto de entrenamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_estimator=[]\n",
    "list_estimator.append(modelF_100)\n",
    "list_estimator.append(modelF_1000)\n",
    "list_estimator.append(modelB_1000)\n",
    "list_predictions = [estimator.predict(x_test) for estimator in list_estimator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 5, 6, ..., 6, 6, 6]),\n",
       " array([5, 6, 6, ..., 6, 6, 6]),\n",
       " array([5, 6, 6, ..., 6, 6, 6])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_predictions[0])):\n",
    "    if ( (list_predictions[0][i] - math.floor(list_predictions[0][i])) > 0.5 ):\n",
    "        list_predictions[0][i] = math.ceil(list_predictions[0][i])\n",
    "    else:\n",
    "        list_predictions[0][i] = math.floor(list_predictions[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 5, 6, ..., 6, 6, 6]),\n",
       " array([5, 6, 6, ..., 6, 6, 6]),\n",
       " array([5, 6, 6, ..., 6, 6, 6])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304\n",
      "1304\n",
      "1304\n"
     ]
    }
   ],
   "source": [
    "print len(list_predictions[0])\n",
    "print len(list_predictions[1])\n",
    "print len(list_predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "solucion=[]\n",
    "for i in range(len(list_predictions[0])):\n",
    "    count=[0,0,0,0,0,0,0,0,0,0,0];\n",
    "    count[int(list_predictions[0][i])] +=1\n",
    "    count[list_predictions[1][i]] +=1\n",
    "    count[list_predictions[2][i]] +=1\n",
    "    solucion.append(count.index(max(count)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score: ', 0.68634969325153372)\n"
     ]
    }
   ],
   "source": [
    "print(\"f1.score: \",f1_score(y_true=y_test,y_pred=solucion,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que al combinar los mejores modelos, se supera el mejor de ellos,por lo cual si vale la pena combinar para mejorar el f1.score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">k) Utilice la técnica de ensamblado para seleccionar características, para ésto defina un criterio para estimar la importancia de los distintos atributos en el ensamblado, impleméntelo sobre alguno de los ensambladores entrenados para resolver el problema definido en b). Realice un ranking de importancia de atributos y seleccione las  𝑘  características más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score train: ', 1.0)\n",
      "('f1.score test: ', 0.68634969325153372)\n"
     ]
    }
   ],
   "source": [
    "modelF_1000 = RandomForestClassifier(n_estimators=1000, max_depth=10000,n_jobs=-1)\n",
    "modelF_1000.set_params(warm_start=True,oob_score=True)\n",
    "modelF_1000.fit(x_train,y_train.values)\n",
    "\n",
    "print(\"f1.score train: \",f1_score(y_true=y_train,y_pred=y_predF_1000,average=\"micro\"))\n",
    "print(\"f1.score test: \",f1_score(y_true=y_test,y_pred=PrediccionF_1000,average=\"micro\"))\n",
    "\n",
    "y_pred = modelF_1000.predict(x_test)\n",
    "D1000= f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(\n",
    "                        modelF_1000.feature_importances_,\n",
    "                        index = x_train.columns,\n",
    "                        columns=['importance']\n",
    "                        ).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>0.123006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.102088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>0.098506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>0.088940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.086792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>0.086580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>0.085363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.084608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>0.084254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.079150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>0.075032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      importance\n",
       "alcohol                 0.123006\n",
       "density                 0.102088\n",
       "volatile acidity        0.098506\n",
       "total sulfur dioxide    0.088940\n",
       "chlorides               0.086792\n",
       "sulphates               0.086580\n",
       "free sulfur dioxide     0.085363\n",
       "residual sugar          0.084608\n",
       "pH                      0.084254\n",
       "citric acid             0.079150\n",
       "fixed acidity           0.075032\n",
       "red                     0.002876\n",
       "white                   0.002804"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si fuera posible esto se podria comparar con la opinion de un experto para ver que tan alejado estamos de la realidad y podemos ver que las variables red y white no influyen mucho, esto es debido a que son dependientes entre ellas y que un vino puede ser malo o bueno sin importar de que tipo sea(tinto o blanco)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">l) Entrene la máquina de aprendizaje definida en h) sobre las  𝑘  carecterísticas derivadas del punto anterior ¿Mejora los resultados sobre ésta máquina de aprendizaje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1.score test sin drop: ', 0.68174846625766872)\n",
      "('f1.score test con drop: ', 0.68251533742331283)\n"
     ]
    }
   ],
   "source": [
    "x_train2 = x_train.drop(columns=['red',\n",
    "                                ])\n",
    "x_test2  = x_test.drop(columns=['red',\n",
    "                                ])\n",
    "\n",
    "\n",
    "model2F_100 = RandomForestClassifier(n_estimators=100, max_depth=10000,n_jobs=-1)\n",
    "model2F_100.set_params(warm_start=True,oob_score=True)\n",
    "model2F_100.fit(x_train2,y_train.values)\n",
    "Prediccion2F_100=model2F_100.predict(x_test2)\n",
    "print(\"f1.score test sin drop: \",f1_score(y_true=y_test,y_pred=PrediccionF_100,average=\"micro\"))\n",
    "print(\"f1.score test con drop: \",f1_score(y_true=y_test,y_pred=Prediccion2F_100,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print 0.68251533742331283 > 0.68174846625766872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al quitar k etiquetas desde las menos relevantes a las más relevantes en general no mejoran la predicción del modelo de regresión lineal propuesto en H ya que cada característica es capaz de sumar o definir en cierta medida la calidad del vino. Sin embargo fué posible apreciar una etiqueta mal definida durante la definición de la representación del problema ya que se asignaron columnas \"red\" y \"white\" en las que se indicaba un 1 o un 0 si el vino correspondía a alguna de esas clases. Sin embargo fue posible mejorar el modelo en una pequeña medida al quitar la columna \"red\" y dejar solo la columna \"white\". Lo anterior cambia la representación ya que ahora en la columna \"white\" se indicará con 1 si el vino es blanco y con 0 si el vino es red, esto se debia a la dependencia de las dos variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
